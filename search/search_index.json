{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#hello","title":"Hello","text":"<p>My name is Will Dean. My background is in Statistics and have been working as a Data Scientist in the transportation and real estate industries. </p> <p>I love to learn and collaborate so feel free to connect with me! </p> <p></p>"},{"location":"appearances/","title":"Appearances","text":""},{"location":"appearances/#appearances","title":"Appearances","text":""},{"location":"appearances/#the-python-exchange-streamline-python-package-release-with-github","title":"The Python Exchange: Streamline Python Package Release with GitHub","text":"<p>A talk at The Python Exchange about streamlining the release process for Python packages using GitHub Actions.</p>"},{"location":"appearances/#mmm-with-pymc-marketing-and-databricks","title":"MMM with PyMC-Marketing and Databricks","text":"<p>A presentation on how to use the PyMC-Marketing library with Databricks for Marketing Mix Modeling.</p>"},{"location":"appearances/#learning-bayesian-statistics","title":"Learning Bayesian Statistics","text":"<p>A discussion about learning Bayesian statistics and its applications.</p>"},{"location":"appearances/#latent-calendar-modeling-weekly-behavior-with-latent-components-with-william-dean","title":"Latent Calendar: Modeling Weekly Behavior with Latent Components with William Dean","text":"<p>A presentation on the latent calendar model for analyzing weekly behavior patterns.</p>"},{"location":"appearances/#exploring-pymc-and-bayesian-approaches-with-will-dean","title":"Exploring PyMC and Bayesian Approaches with Will Dean","text":"<p>A conversation about PyMC and various Bayesian approaches.</p>"},{"location":"open-source/","title":"Open Source","text":""},{"location":"open-source/#open-source-contributions","title":"Open Source Contributions","text":"<p>I really enjoy tinkering with code and contributing to open source projects. Here are some of the repositories I have created and contributed to.</p> <p>Created by me:  </p> <ul> <li>latent-calendar: Analyze and model weekly calendar distributions using latent components</li> <li>conjugate-models: A Python package for Bayesian Conjugate Models</li> <li>frame-search: A Python package for searching DataFrames with GitHub search-like syntax</li> <li>pandas-bootstrap: A Python package for bootstrapping in Pandas</li> <li>lyft-bikes: A Python client of Lyft's Bike Share Data</li> <li>closing-labels: GitHub Action to copy labels from any Issues closed by a pull request into the pull request itself</li> <li>but-better: Your Code, But Better - Python decorator for long running functions</li> </ul> <p>Core contributor to: </p> <ul> <li>PyMC-Marketing: Bayesian marketing toolbox in PyMC</li> <li>octo.nvim: A Neovim plugin for interacting with GitHub Issues, Pull Requests, and other objects</li> </ul> <p>Contributed to:</p> <ul> <li>PyMC: Various Issues and Pull Requests</li> <li>PyPika: Various Issues and Pull Requests</li> <li>Small features, bug fix, of documentation PRs <ul> <li>bambi</li> <li>formulae</li> <li>aesara</li> <li>mkdocs-git-revision-date-localized-plugin</li> <li>timeseers</li> <li>folium</li> <li>branca</li> </ul> </li> </ul>"},{"location":"blog/","title":"Blog Posts","text":""},{"location":"blog/#blog-posts","title":"Blog Posts","text":"<p>Here are some things that I've found interesting and have written about.</p>"},{"location":"blog/#tag:bayesian-statistics","title":"Bayesian Statistics","text":"<ul> <li>            Conjugate Priors          </li> </ul>"},{"location":"blog/#tag:config-files","title":"Config Files","text":"<ul> <li>            Pydantic for Configs          </li> </ul>"},{"location":"blog/#tag:data-analysis","title":"Data Analysis","text":"<ul> <li>            Extending Pandas          </li> <li>            Iteration Pattern          </li> <li>            Working within Docker container          </li> </ul>"},{"location":"blog/#tag:design-patterns","title":"Design Patterns","text":"<ul> <li>            Iteration Pattern          </li> <li>            Strategy Pattern for Flexible Solutions          </li> </ul>"},{"location":"blog/#tag:development","title":"Development","text":"<ul> <li>            Working within Docker container          </li> </ul>"},{"location":"blog/#tag:docker","title":"Docker","text":"<ul> <li>            Working within Docker container          </li> </ul>"},{"location":"blog/#tag:documentation","title":"Documentation","text":"<ul> <li>            Automate marimo Notebook Documentation          </li> <li>            MkDocs and GitHub Pages          </li> </ul>"},{"location":"blog/#tag:github-actions","title":"GitHub Actions","text":"<ul> <li>            Automate marimo Notebook Documentation          </li> </ul>"},{"location":"blog/#tag:marketing","title":"Marketing","text":"<ul> <li>            Strategy Pattern for Flexible Solutions          </li> </ul>"},{"location":"blog/#tag:pandas","title":"Pandas","text":"<ul> <li>            Extending Pandas          </li> </ul>"},{"location":"blog/#tag:pymc","title":"PyMC","text":"<ul> <li>            Strategy Pattern for Flexible Solutions          </li> </ul>"},{"location":"blog/#tag:python","title":"Python","text":"<ul> <li>            Automate marimo Notebook Documentation          </li> <li>            Conjugate Priors          </li> <li>            Debugging with pytest          </li> <li>            Extending Pandas          </li> <li>            Iteration Pattern          </li> <li>            MkDocs and GitHub Pages          </li> <li>            My Favorite Python builtin: pathlib          </li> <li>            Pydantic for Configs          </li> <li>            Strategy Pattern for Flexible Solutions          </li> </ul>"},{"location":"blog/#tag:standard-library","title":"Standard Library","text":"<ul> <li>            My Favorite Python builtin: pathlib          </li> </ul>"},{"location":"blog/#tag:testing","title":"Testing","text":"<ul> <li>            Debugging with pytest          </li> </ul>"},{"location":"blog/posts/2022/pathlib/","title":"My Favorite Python builtin: pathlib","text":"","tags":["Python","Standard Library"]},{"location":"blog/posts/2022/pathlib/#my-favorite-python-builtin-pathlib","title":"My Favorite Python builtin: pathlib","text":"<p>When working with python and data, file names quickly become a pain for many reasons. </p> <p>Firstly, files quickly add up. This could be from raw data, created processed data, config files, results including models and vizualizations. A lot more files are being worked with than initially thought, so not being organized can be quickly overwhelming.</p> <p>Secondly \u2014 and this may be a personal problem for me \u2014 start adding lengthy, hard-coded values for file names just becomes painful. Sometimes I even find it difficult to want to start working scripts.</p> <p>Of course, using strings can work, but this leads to working with many of the functions from the <code>os</code> and <code>os.path</code> modules. Who wants so many imports too? </p> <pre><code>import os \nfrom os.path import join\n\nCURRENT_DIR: str = os.getcwd()\n\nfile_name: str = \"my-data.csv\"\n\ndata_file = join(CURRENT_DIR, file_name)\n\nprint(f\"The file {data_file} exists: {os.path.isfile(data_file)}\")\n</code></pre> <p>Now, hand off your scripts to someone on a Window's machine. All your hard-coded paths aren't even in the right format now and I'm feeling total regrets for ever starting the project .</p>","tags":["Python","Standard Library"]},{"location":"blog/posts/2022/pathlib/#quick-start","title":"Quick Start","text":"<p>Using a <code>pathlib.Path</code> instance instead of a string is meant to be intuitive. </p> <p>That is, many actions like creating file, checking stats, relative location, etc are just methods of a <code>Path</code> instance. Other things like file name, suffix, or parents are attributes of the instance. Forget all the imports, working with <code>Path</code> object takes advantage of OOP design.</p> <p>When working in a python file, the <code>__file__</code> variable can be utilized in order to find out where the current location is. No need to hard code or think about relative paths.</p> current-file.py<pre><code>from pathlib import Path \n\n\nHERE = Path(__file__).parent\n\n# New file next to \"current-file.py\"\nnew_file = HERE / \"new-file.txt\"\n\nif not new_file.exists(): \n    new_file.touch()\n\nnew_file.write_text(\"Writing some text to the new file!\")\n\nRESULTS_DIR = HERE / \"results\"\nRESULTS_DIR.mkdir()\n\n# Some processing\n...\noverride: bool\nresult_file = RESULTS_DIR / \"results-file.csv\"\nif results_file.exists() and not override: \n    msg = \"We don't want to override this file!\"\n    raise ValueError(msg)\n</code></pre> <p>I find the interface very intuitive and cool thing here is that this will work on the machine regardless of the operating system. </p>","tags":["Python","Standard Library"]},{"location":"blog/posts/2022/pathlib/#using-a-data-folder","title":"Using a data folder","text":"<p>I often have a <code>DATA_DIR</code> constant for many of my projects which refers to a folder <code>data</code> off the root of my project. </p> <p>In the file system, that would look like this:</p> <pre><code>my_module/\n    ...\ndata/\n    ...\nREADME.md\n</code></pre> <p>This is an easy setup and saves a lot of headaches in the future.</p> my_module/utils.py<pre><code>from pathlib import Path\n\nDATA_DIR = Path(__file__)\nif not DATA_DIR.exists(): \n    DATA_DIR.mkdir()\n</code></pre> <p>As long as I am working with my python module, I don't have to worry about much more than the file names I want.</p> <pre><code>from my_module import utils\n\nfile: Path = utils.DATA_DIR / \"raw\" / \"my-data.csv\"\n</code></pre>","tags":["Python","Standard Library"]},{"location":"blog/posts/2022/pathlib/#additional-folders","title":"Additional folders","text":"<p>I often extend this to include other folders I will likely have based on a project. This might be a folder <code>data/raw</code>, <code>data/results</code>, or even a <code>configs</code> dir.</p> <p>This depends on the project but all of this is with the goal of being as organized as possible from the start. </p> <p>I recently watched this related video on naming files and found it useful. </p>","tags":["Python","Standard Library"]},{"location":"blog/posts/2022/pathlib/#working-with-s3-locations","title":"Working with S3 locations","text":"<p>I learned about this python package while working with S3 paths on AWS. I haven't personally used but I think it looks promising and would provide a similar enjoyable experience as the <code>pathlib</code> module.</p>","tags":["Python","Standard Library"]},{"location":"blog/posts/2022/pathlib/#conclusion","title":"Conclusion","text":"<p>Taming all the files you are working with is never an easy battle. However, I find using <code>pathlib</code> makes the process just a little more enjoyable.</p>","tags":["Python","Standard Library"]},{"location":"blog/posts/2022/pydantic-configs/","title":"Pydantic for Configs","text":"","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#using-pydantic-for-configs","title":"Using Pydantic for Configs","text":"<p>Tip</p> <p>It might be helpful to know a bit about the pydantic library and its functionality. Luckily, the docs are very good. They are linked below.</p> <p>I discovered the pydantic library when I first started using Typer and FastAPI and quickly found the library very useful for other reasons.</p> <p>One use case I've found very helpful is when making config files for python scripts. </p> <p>There is clear benefit to using configs when writing python code: Variables can be changed without having to edit the python file itself. But by also using pydantic you get the additional benefits provided from the library. </p>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#clearly-define-the-structure-of-the-config","title":"Clearly Define the Structure of the Config","text":"<p>When working with configs, I often find it confusing to what all the possible supported settings are. However, if you define a <code>Config</code> class from the <code>pydantic.BaseModel</code>, you see clear structure for what you are working with. </p> <p>For instance, a project which is working with some input, output, and some additional configuration settings might look like this.</p> <pre><code>from pydantic import BaseModel\n\nfrom pathlib import Path\nfrom typing import Dict, Any\n\n\nclass Config(BaseModel):\n    # Input  \n    input_location: Path\n    # Result location and file name\n    results_dir: Path\n    results_file_name: str\n    # Some additional configurations\n    plotting_kwargs: Dict[str, Any] = {}\n</code></pre> <p>This data can come from many sources but a YAML configuration file for this structure might look like this:</p> config.yaml<pre><code>input_location: ./data/input_data.csv\nresults_dir: ./results/\nresults_file_name: my_first_run.png\nplotting_kwargs: \n    alpha: 0.5\n</code></pre> <p>By looking at the class structure, it is clear that some input and output information is required and additional plotting information is optional. Not only that, the user gets an understanding of what format the data should be in.</p>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#adding-hierarchy","title":"Adding Hierarchy","text":"<p>If the config starts to get too large, I've found splitting up into different sections to be very helpful. This can be sections for inputs, output, related setting, etc. A more organized YAML could look like this:</p> <pre><code>input: \n    file: some-input-file.csv\noutput: \n    base_dir: some-directory-to-save\n    result_name: some-file-name.png\n</code></pre> <p>In order to support this structure, each one of the sections would be its own class in the defined pydantic model. </p> <pre><code>class InputSetting(BaseModel): \n    file: Path \n\n\nclass OutputSetting(BaseModel): \n    base_dir: Path\n    results_name: str \n\n\nclass Config(BaseModel): \n    \"\"\"Class version of the full config file\"\"\"\n    input: InputSettings\n    output: OutputSettings\n</code></pre> <p>This allows for like items to be broken up in a logical way and support complicated configuration options.</p>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#additional-functionality","title":"Additional Functionality","text":"<p>Since all of these configs items are python classes, additional functionality can be added to them with methods and attributes. This add to the cohesion of the code by putting similiar methods together.</p> <p>For instance, if there is some type of connection settings, and adding additional functionality to load data might be helpful to group together. </p> <pre><code>import pandas as pd\n\n\nclass DataBaseSettings(BaseModel): \n    schema: str \n    table: str \n\n    def is_connected(self) -&gt; bool: \n        \"\"\"Determine if connection exists.\"\"\"\n\n    def read_table(self) -&gt; pd.DataFrame: \n        \"\"\"Return the table from the database.\"\"\"\n\n\nclass Config(BaseModel): \n    database: DataBaseSettings\n</code></pre>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#generalization-and-extensions","title":"Generalization and Extensions","text":"<p>Because of the powerful structure parsing of pydantics, we can extend our configs very easily by providing abstractions to our data models.</p> <p>For instance, if multiple different data sources want to be supported, then that can be reflected in our configuration.</p> <pre><code>class CSVSettings(BaseModel): \n    \"\"\"Data source that is csv format.\"\"\"\n    location: Path\n\n    def read_table(self) -&gt; pd.DataFrame: \n        \"\"\"Read from csv file.\"\"\"\n\n\nclass Config(BaseModel): \n    \"\"\"Generalized configuration.\"\"\"\n    source: CSVSettings | DataBaseSettings\n</code></pre> <p>Because pydantic will be able to understand these structural differences, we are able to change our config file accordingly. </p> <pre><code>---\nsource: \n    location: data/some-local-data.csv\n---\nsource: \n    schema: my-schema\n    table: my-table\n</code></pre> <p>When the config is parsed into an instance, the common interface can be leveraged in the code while also providing flexibility in the settings.</p>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#reusability","title":"Reusability","text":"<p>If there are multiple configuration files required for a project, there will often be overlapping configuration elements. By structuring the code in the hierarchical manner, different classes can be reused in order to simplify our interface.</p> <pre><code>class RunConfig(BaseModel): \n    \"\"\"Running and saving off a model.\"\"\"\n    input_settings: InputSettings\n    model_settings: ModelSettings\n    results: ResultsLocation\n\n\nclass InterpretationConfig(BaseModel): \n    \"\"\"Loading and interpreting model.\"\"\"\n    results: ResultsLocation\n</code></pre> <p>The <code>ResultsLocation</code> class might be useful in multiple configuration files here because it is used to both save and load data.</p>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#limiting-options","title":"Limiting Options","text":"<p>If you want to limit options a variable can take, an <code>enum.Enum</code> type can be used to enforce only a set number of choices.</p> <p>This provides some checking at config parsing time which can give you some quick feedback.</p> <pre><code>from enum import Enum \n\n\nclass Difficulties(Enum): \n    EASY: str = \"easy\"\n    MEDIUM: str = \"medium\"\n    HARD: str = \"hard\"\n\n\nclass Config(BaseModel): \n    difficulty: Difficulties\n</code></pre> <p>Or like the other example above, a union of types can be expressed.</p>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#additional-validation","title":"Additional Validation","text":"<p>Pydantic provides a bunch of additional data validation which can provide some runtime checks to your configuration.</p> <p>If you are used to using dataclasses too, the dataclasses submodule can be very helpful in order to add some additional checks on the configs at runtime as well.</p> <pre><code>from pydantic.dataclasses import dataclass\n\n\n@dataclass\nclass ResultSettings: \n    results_dir: Path\n    file_name: str \n    override: bool = False\n\n    def __post_init__(self) -&gt; None: \n        if not self.results_dir.exists(): \n            self.results_dir.mkdir()\n\n        save_location = self.results_dir / self.file_name\n        if save_location.exists() and not override: \n            msg = f\"The results already exists. Not running {save_location}\" \n            raise ValueError(msg)\n</code></pre>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#class-implementation","title":"Class Implementation","text":"<p>I often add a lightweight class implementation when working with YAML configs. The goal here is to add an additional method to the pydantic <code>BaseModel</code> in order to easily load different config files.</p> yaml_base_model.py<pre><code>from pydantic import BaseModel\nimport yaml\n\nfrom pathlib import Path\n\n\nclass YamlBaseModel(BaseModel)\n    @classmethod\n    def from_yaml(cls, file: str | Path) -&gt; YamlBaseModel: \n        file = Path(file)\n\n        with open(file, \"r\") as f: \n            data = yaml.safe_load(f)\n\n            return cls.parse_obj(data)\n</code></pre> <p>Then when defining a config file, this will be the class inherited from. Making it clear which define the structure of config files and which are just parts of a larger configuration.</p> <pre><code>class ModelSettings(BaseModel): \n    \"\"\"Won't be a config file but will be part of some larger configuration.\"\"\"\n    folds: int \n    method: str\n    ...\n\n\nclass RunConfig(YamlBaseModel):\n    \"\"\"Some YAML config file will have this structure.\"\"\"\n    input: InputSettings\n    model_settings: ModelSettings\n</code></pre> <p>This allows for easy construction of a config object and can be used accordingly.</p> run_script.py<pre><code>if __name__ == \"__main__\": \n    config = RunConfig.from_yaml(\"./configs/run-config.yaml\")\n\n    data = config.input.load_data()\n</code></pre> <p>Find the gist of this here with an additional example.</p> <p>Prefer TOML Configs? Can imagine similar support for TOML configs (especially with latest support in python 3.11). Same goes with some additional formats too.</p>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#alternative-comparison","title":"Alternative Comparison","text":"<p>I have an example where: </p> <ol> <li>Some data will be loaded in</li> <li>Some model with configuration is loaded in</li> <li>The model is trained and saves:<ol> <li>Logging information</li> <li>Trained model</li> </ol> </li> </ol> <p>The three implementations will go from worst to best.</p>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#1-hard-coding-constants","title":"1. Hard Coding Constants","text":"<p>This is the worst case implementation since any changes happen to happen in the python file itself. </p> <p>Not only that, but some of the hard-coded values are at the end of the file. That could be hard to sift through if it was a larger file!</p> <pre><code>from my_module import utils, model, data\n\nif __name__ == \"__main__\": \n    df = data.load_data(\n        utils.RAW_DATA_DIR / \"training-data.csv\"\n    )\n\n    my_model_config = {\n        ...\n    }\n    my_model = model.MyModel(**my_model_config)\n    my_model.train(df)\n\n    my_model.save_logs(\n        utils.LOGGING_DIR / \"logging.txt\"\n    )\n\n    my_model.save_model(\n        utils.MODEL_DIR / \"my-model.pkl\"\n    )\n</code></pre>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#2-using-unstructured-configs","title":"2. Using Unstructured Configs","text":"<p>This introduces a yaml config file which separates all the changing variables from the python file itself. However, it is not totally clear what are all the options available.</p> <p>I do think this is a large improvement though!</p> config.yaml<pre><code>training_data: training-data.csv\nmodel_config: \n    folds: 5\n    random_seed: 42\n    ...\nlogging_name: logging.txt\nmodel_name: my-model.pkl\n</code></pre> <pre><code>from typing import Dict, Any\n\nif __name__ == \"__main__\": \n    config: Dict[str, Any] = utils.load_config(\"config.yaml\")\n\n    df = data.load_data(\n        utils.RAW_DATA_DIR / config[\"training_data\"]\n    )\n\n    my_model = model.MyModel(**config[\"model_config\"])\n    my_model.train(df)\n\n    my_model.save_logs(\n        utils.LOGGING_DIR / config[\"logging_name\"]\n    )\n\n    my_model.save_model(\n        utils.MODEL_DIR / config[\"model_name\"]\n    )\n</code></pre>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#3-config-with-pydantic","title":"3. Config with Pydantic","text":"<p>This might be a way to implement with pydantic.</p> my_module/config.py<pre><code>from my_module import utils \n\n\nclass Source: \n    training_data: str \n    raw_data: Path = utils.RAW_DATA_DIR\n\n    def load_data(self) -&gt; pd.DataFrame: \n        ...\n\nclass ModelConfig(BaseModel): \n    folds: int = 5\n    random_seed: int = 42\n\n    def slugify_config(self, file_base: Path) -&gt; str: \n        \"\"\"Helper for file naming.\"\"\"\n\n\nclass Artifacts(BaseModel): \n    logging_name: str \n    model_name: str \n    logging_dir: Path = utils.LOGGING_DIR\n    model_dir: Path = utils.MODEL_DIR\n\n\nclass Config(YamlBaseModel): \n    training_source: Source\n    model_config: ModelConfig\n    model_artifacts: Artifacts \n</code></pre> config.yaml<pre><code>training_source: \n    training_data: training-data.csv\nmodel_config: \n    folds: 10\n    random_seed: 1\nartifacts: \n    logging_name: logging.txt\n    model_name: my-model.pkl\n</code></pre> <pre><code>if __name__ == \"__main__\": \n    config: Config = Config.from_yaml(\"config.yaml\")\n\n    df = config.source.load_data()\n\n    my_model = model.MyModel(config.model_config)\n    my_model.fit(df)\n\n    my_model.save_artifacts(\n        config.model_config, \n        config.model_artifacts\n    )\n</code></pre> <p>This is clearly the most explicit version of the three. However, there are a lot of benefits for doing so. </p> <ol> <li>Clearly defined configuration leads to: <ol> <li>Known functionality from class definitions</li> <li>Related functionality sticks together</li> </ol> </li> <li>Shorter code in script because:<ol> <li>OOP structuring </li> <li>Ability to work with default values</li> </ol> </li> <li>Able to be extended if desired like: <ol> <li>Better file naming using config information</li> </ol> </li> </ol>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/pydantic-configs/#conclusion","title":"Conclusion","text":"<p>Overall, I've found defining configs with pydantic in mind very useful. It can be super quick to do, provide a lot more structure and understanding to the config settings, and leverage the powerful parsing validation from the library. Give it a try!</p>","tags":["Python","Config Files"]},{"location":"blog/posts/2022/site-setup/","title":"MkDocs and GitHub Pages","text":"","tags":["Python","Documentation"]},{"location":"blog/posts/2022/site-setup/#mkdocs-and-github-pages","title":"MkDocs and GitHub Pages","text":"<p>This website was created using <code>MkDocs</code>, surrounding python packages like mkdocs-material, and is served with GitHub pages. </p>","tags":["Python","Documentation"]},{"location":"blog/posts/2022/site-setup/#about-mkdocs-and-its-plugins","title":"About MkDocs and its plugins","text":"<p>The MkDocs library makes it easy to create documentation from just markdown and YAML files.</p> <p>With just a few commands, you can quickly iterate with your content and make some great looking webpages.</p> <p>There is also a handful plugin libraries for MkDocs that allow for additional customization. For instance, the theme used here is from the <code>mkdocs-material</code> package. Makes it so easy to have a good looking site on all platforms (especially for people like me who don't make user interfaces often ).</p>","tags":["Python","Documentation"]},{"location":"blog/posts/2022/site-setup/#creating-the-content-and-theme","title":"Creating the Content and Theme","text":"<p>MkDocs really makes it so easy to set up a page. The Getting Started Section allows you to get started.</p> <p>The simpliest project setup will just be two files: </p> <p> </p> <p>The themes and capabilities were inspired from the mkdocs-material site and exploring its mkdocs.yml helped implement mine.</p>","tags":["Python","Documentation"]},{"location":"blog/posts/2022/site-setup/#serving-with-github-pages","title":"Serving with GitHub Pages","text":"<p>GitHub allows you to set up a site for free. If the repo is named <code>&lt;username&gt;.github.io</code>, then it will be hosted at <code>https://&lt;username&gt;.github.io</code>. </p> <p>Tip</p> <p>If you deploy with GitHub pages in any other repo, the docs with be hosted at <code>https://&lt;username&gt;.github.io/&lt;repo-name&gt;</code>. Super cool and would like to use in the future to make quick project documentation!</p> <p>All the code to create this site is on my GitHub here.</p>","tags":["Python","Documentation"]},{"location":"blog/posts/2022/site-setup/#github-action","title":"GitHub Action","text":"<p>The mkdocs-material documentation has the steps in order to set up your GitHub Action for deploy. Those steps are found here.</p>","tags":["Python","Documentation"]},{"location":"blog/posts/2022/site-setup/#github-page-settings","title":"GitHub Page Settings","text":"<p>I had some hiccups while setting up the site. Initially, only my README was being served at https://williambdean.github.io. However, the default \"Deploy from a branch\" setting in the GitHub Pages section was on the <code>main</code> branch which only had the README. </p> <p>Note</p> <p>Running <code>mkdocs gh-deploy</code> pushes all the html files to <code>gh-pages</code> branch in my repo so that branch needed to be selected in the Settings.</p> <p>Below are Settings section that worked for my deployment.</p> <p></p>","tags":["Python","Documentation"]},{"location":"blog/posts/2022/site-setup/#helpful-links","title":"Helpful Links","text":"<p>I found these links helpful while setting up this site and writing this post.</p> <ul> <li>MkDocs</li> <li>MkDocs Material</li> <li>GitHub Pages</li> </ul>","tags":["Python","Documentation"]},{"location":"blog/posts/2023/conjugate-priors/","title":"Conjugate Priors","text":"","tags":["Python","Bayesian Statistics"]},{"location":"blog/posts/2023/conjugate-priors/#conjugate-priors","title":"Conjugate Priors","text":"<p>Bayesian statistics is a great way to think about data under the uncertainty of model parameters and I've found conjugate priors to be a good way to get started with some problems. </p>","tags":["Python","Bayesian Statistics"]},{"location":"blog/posts/2023/conjugate-priors/#what-is-a-conjugate-prior","title":"What is a Conjugate Prior?","text":"<p>A conjugate prior is a prior distribution that is in the same family as the posterior distribution. This is a mathematic convenience that makes it easier to calculate the posterior distribution, often just with a simple addition to the parameters of the prior distribution using the observed data. </p> <p>For instance, if we have a Bernoulli distribution with a single unknown success rate, a Beta prior on the success rate results in a posterior distribution that is also a Beta distribution. The posterior distribution is just the prior distribution with the addition of the number of successes and failures.</p> Get Posterior Distribution of Bernoulli Distribution with Beta Prior<pre><code>from conjugate.distributions import Beta\nfrom conjugate.models import binomial_beta\n\nN = 10\nX = 4\n\nprior = Beta(alpha=1, beta=1)\nposterior: Beta = binomial_beta(n=N, x=X, beta_prior=prior)\n</code></pre> <p>Often times as well, the posterior predictive distribution in in a closed form distribution too. This provides new, alternative data sets. For instance, the posterior predictive distribution of a Bernoulli distribution with a Beta prior is a Beta-Binomial distribution.</p> Get Posterior Predictive Distribution of Bernoulli Distribution with Beta Prior<pre><code>from conjugate.distributions import BetaBinomial\nfrom conjugate.models import binomial_beta_posterior_predictive\n\nposterior_predictive: BetaBinomial = binomial_beta_posterior_predictive(\n    n=N, \n    beta=posterior\n) \n</code></pre> <p>Having a closed form distribution for the posterior distribution and posterior predictive distribution can be useful to quickly assess the data, make decisions, and communicate more than just a parameter point estimate.</p> <p>Tip</p> <p>The prior predictive distribution is also a Beta-Binomial distribution. Just the posterior predictive with prior info. This provides the results we expect to see before we see the actual data.</p> <p>Below is visualization of 10 trials, before and after we see the data.</p> <p></p>","tags":["Python","Bayesian Statistics"]},{"location":"blog/posts/2023/conjugate-priors/#common-conjugate-models","title":"Common Conjugate Models","text":"<p>Many common distributions like the Bernoulli, Poisson, or Normal distributions have conjugate models. They show up a lot in the single parameter distributions or where all but one parameter is known. </p> <p>Wikipedia provides table of common conjugate models here.</p>","tags":["Python","Bayesian Statistics"]},{"location":"blog/posts/2023/conjugate-priors/#reducing-problem-to-a-conjugate-model","title":"Reducing Problem to a Conjugate Model","text":"<p>Some data problems scream out these common distributions. For instance,</p> <ul> <li>Count data can be modeled with a Poisson distribution</li> <li>Binary data can be modeled with a Bernoulli distribution</li> <li>Number of successes / single outcomes in a fixed number of trials / attempts can be modeled with a Binomial distribution</li> <li>Sum of independent identical distributed variables can be modeled with a Normal distribution</li> </ul> <p>If not, many questions on the data can be reduced to one of these common distributions. </p> <p>For instance, binning data into two groups can be modeled with a Bernoulli or Binomial distribution. This can be helpful in understanding the tail of a distribution. </p> <p>Knowing common relationships between distributions are useful to understand as well. </p>","tags":["Python","Bayesian Statistics"]},{"location":"blog/posts/2023/conjugate-priors/#why-use-conjugate-priors","title":"Why Use Conjugate Priors?","text":"<p>Conjugate priors can be a starting point due to their simplicity.</p> <p>The availability of quantiles and moments can be useful to understand the data, even if the observed data falls outside of the posterior and posterior predictive distributions.</p> <p>They can give a sense of fit or the lack of.</p>","tags":["Python","Bayesian Statistics"]},{"location":"blog/posts/2023/conjugate-priors/#using-conjugate-priors-in-different-settings","title":"Using Conjugate Priors in Different Settings","text":"<p>Since there is a closed for the posterior distribution, these models could also be implemented in SQL and moments could be calculated in SQL as well. This could be useful for large datasets that are too large to fit in memory and could be useful for a quick analysis backed with statistical theory.</p> <p>Though these distributions are simple, they can be applied at a very granular level. For instance, a single user's click through rate could be modeled with a Bernoulli distribution. This could be useful to understand the uncertainty of a single user's click through rate. A quick win for user level personalization could be to show the user the content with the highest posterior predictive distribution. Or, some relevant reward function could be used to determine the best content to show the user.</p>","tags":["Python","Bayesian Statistics"]},{"location":"blog/posts/2023/conjugate-priors/#summary","title":"Summary","text":"<p>Conjugate priors are a great way to get started with Bayesian statistics. They provide a closed form posterior distribution and, often, posterior predictive distribution. This can be useful to understand the data and make decisions.</p> <p>They are good for simple models and can be applied at a granular level.</p> <p>If you're interested in trying out in python or want to see more examples, check out my repo and docs to use conjugate priors. </p>","tags":["Python","Bayesian Statistics"]},{"location":"blog/posts/2023/docker-container/","title":"Working within Docker container","text":"","tags":["Data Analysis","Development","Docker"]},{"location":"blog/posts/2023/docker-container/#working-within-docker-container","title":"Working within Docker container","text":"<p>This has been my workflow to iteractively work from within a running container. </p> <p>The goal here is to still rely on the local machine for editing files but to rely on the environment from the container for running any code. </p> <p>This simple setup provides: </p> <ul> <li>Setup of environment regardless of current system dependencies</li> <li>Flexible and reproducible environments</li> <li>Access to local files from container's shell </li> </ul> <p>In short, the process is as follows: </p> <ol> <li>Find or build base image</li> <li>Open container's shell </li> </ol> <pre><code>IMAGE_NAME=quick-env \ndocker build -t $IMAGE_NAME . \n\ndocker run --rm -it -v $(pwd):/app -w /app --entrypoint bash $IMAGE_NAME\n</code></pre> <p>Below is a breakdown of these commands. </p>","tags":["Data Analysis","Development","Docker"]},{"location":"blog/posts/2023/docker-container/#find-or-build-base-image","title":"Find or build base image","text":"<p>We can use one of the many images on Docker Hub or build off one with a custom <code>Dockerfile</code>. </p> <p>Below has python 3.11 with <code>pandas</code>, <code>matplotlib</code>, and <code>IPython</code>  installed: </p> Dockerfile<pre><code>FROM python:3.11\n\n# Setup the environment to use in terminal\nRUN pip install pandas matplotlib IPython\n</code></pre> <p>Build a new image with <code>docker build -t &lt;image-name&gt; .</code></p> <p>Note</p> <p>Nothing from the local file system is tranferred here as we will still rely on the local file system</p>","tags":["Data Analysis","Development","Docker"]},{"location":"blog/posts/2023/docker-container/#run-containers-shell","title":"Run container's shell","text":"<p>Run this container and enter iteractively into its shell with the following command: </p> <p><code>docker run --rm -it -v $(pwd):/app -w /app --entrypoint bash &lt;image-name&gt;</code></p> <p>These flag shouldn't need to be changed and should work in most cases but can always be customized. </p> <p>The <code>docker run</code> documentation is very thorough but here is some info on the flags used: </p>","tags":["Data Analysis","Development","Docker"]},{"location":"blog/posts/2023/docker-container/#-rm-container-cleanup","title":"<code>--rm</code>: Container cleanup","text":"<p>This is optional flag but is helpful for decluttering after a run.</p>","tags":["Data Analysis","Development","Docker"]},{"location":"blog/posts/2023/docker-container/#-it-interative-terminal","title":"<code>-it</code>: Interative terminal","text":"<p>Use the <code>-i</code> and <code>-t</code> flags (or <code>-it</code> together) in order for running container to use our input and output.  I alway remember this as iteractive terminal</p>","tags":["Data Analysis","Development","Docker"]},{"location":"blog/posts/2023/docker-container/#-v-access-to-local-files","title":"<code>-v</code>: Access to local files","text":"<p>Mount local volume such that changes to local files can be accessed from and by the container. </p> <p>This can be any location on either local or image, but <code>$(pwd):/app</code> usually does the trick.</p> <p>Note</p> <p>The <code>/app</code> location will be created if it doesn't already exist!</p>","tags":["Data Analysis","Development","Docker"]},{"location":"blog/posts/2023/docker-container/#-w-setting-container-working-dir","title":"<code>-w</code>: Setting container working dir","text":"<p>Though this is not required, this will set the local directory for the container to where we mounted our local files. </p>","tags":["Data Analysis","Development","Docker"]},{"location":"blog/posts/2023/docker-container/#-entrypoint-enter-container-terminal","title":"<code>--entrypoint</code>: Enter container terminal","text":"<p>Using <code>docker run</code> doesn't guarantee that a terminal will be kicked off. However, the entrypoint for the image can be overridden during the with the <code>--entrypoint</code> flag. </p> <p>Tip</p> <p>Most of the time the shell will be <code>bash</code> but could be another shell like <code>sh</code></p>","tags":["Data Analysis","Development","Docker"]},{"location":"blog/posts/2023/docker-container/#summary","title":"Summary","text":"<p>Docker makes it easy to create new isolated environments without touching local system dependencies while working with local files.</p> <p>Overall, Docker is awesome </p>","tags":["Data Analysis","Development","Docker"]},{"location":"blog/posts/2023/docker-container/#references","title":"References","text":"<ul> <li>Docker Hub</li> <li><code>docker run</code> Documentation</li> </ul>","tags":["Data Analysis","Development","Docker"]},{"location":"blog/posts/2023/extending-pandas/","title":"Extending Pandas","text":"","tags":["Python","Pandas","Data Analysis"]},{"location":"blog/posts/2023/extending-pandas/#extending-pandas","title":"Extending Pandas","text":"<p>Here is a quick way to make your functionality with pandas objects just as common as using pandas itself and help promote readable code. </p> <p>Our goal here is to make a function that can be used on a pandas object via a self defined attribute. </p> <pre><code>import pandas as pd\nimport my_module\n\ndf = pd.DataFrame(...)\n# Using user defined boot attribute with its get_samples method\ndf_bootstrap: pd.DataFrame = df.boot.get_samples(my_func, B=100)\n</code></pre>","tags":["Python","Pandas","Data Analysis"]},{"location":"blog/posts/2023/extending-pandas/#implementation","title":"Implementation","text":"<p>In order to do this, we need to create a class that extends the pandas object. This is done by using the <code>pd.api.extensions.register_dataframe_accessor</code> decorator. The name we pass will be the name of the attribute we use to access the functionality.</p> <p>Below creates a functions that will bootstrap a function on a DataFrame and will define the <code>boot</code> attribute on a DataFrame with the <code>BootAccessor</code> class. There the bootstrap function is defined as a method on the class.</p> <pre><code>import pandas as pd\n\ndef bootstrap(df: pd.DataFrame, b_func, B: int = 100) -&gt; pd.DataFrame: \n    \"\"\"Bootstrap a function on a DataFrame. \n\n    Adds sample index to the result.\n\n    Args:\n        df (pd.DataFrame): DataFrame to bootstrap\n        b_func (Callable): Function to bootstrap\n        B (int, optional): Number of bootstrap samples. Defaults to 100.\n\n    Returns:\n        pd.DataFrame: DataFrame of bootstrap samples\n\n    \"\"\"\n    return pd.concat([\n        df\n        .sample(frac=1, replace=True)\n        .pipe(b_func)\n        .rename(i)\n        .to_frame() \n        for i in range(B)\n    ], axis=1).T\n\n@pd.api.extensions.register_dataframe_accessor(\"boot\")\nclass BootAccessor: \n    def __init__(self, pandas_obj):\n        self._obj = pandas_obj\n\n    def get_samples(self, b_func, B: int = 100) -&gt; pd.DataFrame: \n        \"\"\"Bootstrap a function on a DataFrame\n\n        Args:\n            b_func (Callable): Function to bootstrap\n            B (int, optional): Number of bootstrap samples. Defaults to 100.\n\n        Returns:\n            pd.DataFrame: DataFrame of bootstrap samples\n\n        \"\"\"\n        return bootstrap(self._obj, b_func=b_func, B=B)\n</code></pre>","tags":["Python","Pandas","Data Analysis"]},{"location":"blog/posts/2023/extending-pandas/#usage","title":"Usage","text":"<p>After import of this module, you can use the <code>boot</code> accessor on any pandas object.</p> <pre><code>import pandas as pd\n\ndf = pd.DataFrame(...)\ntry: \n    df.boot\nexcept AttributeError: \n    pass\n\nimport my_module\n\ndef my_func(df: pd.DataFrame) -&gt; pd.Series: \n    \"\"\"Function to bootstrap\n\n    Args:\n        df (pd.DataFrame): DataFrame to bootstrap\n\n    Returns:\n        pd.Series: mean of the columns\n    \"\"\"\n    return df.mean()\n\ndf.boot.get_samples(b_func=my_func, B=100)\n</code></pre> <p>Though is just a single method, this technique can be used to package up a lot of functionality.</p>","tags":["Python","Pandas","Data Analysis"]},{"location":"blog/posts/2023/extending-pandas/#adding-validation","title":"Adding Validation","text":"<p>The <code>BootAccessor</code> class can be extended to add validation to the DataFrame before the bootstrap is performed. This can be good for checking that the DataFrame has the correct columns or that the values are in the correct range -- or anything else for the use case.</p> <pre><code>@pd.api.extensions.register_dataframe_accessor(\"boot\")\nclass BootAccessor: \n    def __init__(self, pandas_obj):\n        self._validate(pandas_obj)\n        self._obj = pandas_obj\n\n    @staticmethod\n    def _validation(df: pd.DataFrame) -&gt; bool: \n        \"\"\"Validate DataFrame\n\n        Args:\n            df (pd.DataFrame): DataFrame to validate\n\n        Returns:\n            bool: True if DataFrame is valid\n        \"\"\"\n        return True\n</code></pre> <p>A simple addition to add checks to all of your functionality.</p>","tags":["Python","Pandas","Data Analysis"]},{"location":"blog/posts/2023/extending-pandas/#alternatives-conclusion","title":"Alternatives &amp; Conclusion","text":"<p>Using the <code>pipe</code> method on pandas objects is great way to make some readable code, but it can quickly become a bit verbose with imports.</p> <pre><code>from my_module import bootstrap, preprocess_func, postprocess_func, plot_timeseries\n\ndf = pd.DataFrame(...)\ndf_result = (\n    df\n    .pipe(preprocess_func, ...)\n    .pipe(bootstrap, b_func=my_func, B=100)\n    .pipe(postprocess_func, ...)\n    .pipe(plot_timeseries, ...)\n</code></pre> <p>An alternative might look like this</p> <pre><code>import my_module\n\ndf_result = (\n    df\n    .transformations.preprocess(...)\n    .boot.get_samples(b_func=b_func, B=100)\n    .transformations.postprocess(...)\n    .plotting.timeseries(...)\n)\n</code></pre> <p>All in all, it's a quick change to add new functionality the widely used data type and maybe help the user experience.</p>","tags":["Python","Pandas","Data Analysis"]},{"location":"blog/posts/2023/extending-pandas/#resources","title":"Resources","text":"<ul> <li>Pandas User Guide: Extending Pandas</li> <li><code>pandas-bootstrap</code> package</li> </ul>","tags":["Python","Pandas","Data Analysis"]},{"location":"blog/posts/2023/iteration-pattern/","title":"Iteration Pattern","text":"","tags":["Design Patterns","Python","Data Analysis"]},{"location":"blog/posts/2023/iteration-pattern/#iteration-pattern","title":"Iteration Pattern","text":"<p>Make use of python iteration for more generalized code and functionality</p> <pre><code>for value in iterable: \n    process(value)\n</code></pre>","tags":["Design Patterns","Python","Data Analysis"]},{"location":"blog/posts/2023/iteration-pattern/#allowing-iteration-in-python","title":"Allowing iteration in python","text":"<p>Objects can be used in <code>for</code> loop as long as the object is iterable or has a way to iterate over it.</p> <p>This functionality can be defined with with the <code>__iter__</code> and <code>__next__</code> method or a generator. </p>","tags":["Design Patterns","Python","Data Analysis"]},{"location":"blog/posts/2023/iteration-pattern/#1-generator-function","title":"1. Generator Function","text":"<p>A generator is a function that returns an iterator. This is done by using the <code>yield</code> keyword instead of <code>return</code>.</p> Example of generator<pre><code>def generator(): \n    yield 1\n    yield 2\n    yield 3\n\ndef same_generator(): \n    for i in range(1, 4): \n        yield i\n\nfor value in generator():\n    print(value)\n</code></pre> <pre><code>1\n2\n3\n</code></pre> <p>Note</p> <p>Iterators are pretty common in python. i.e. <code>range</code> is an iterator.</p> <p>Alternatives</p> <p>The <code>yield from</code> keyword can be used as an alternative to yield all values from another iterator.</p> Example of yield from<pre><code>def generator(): \n    yield from range(1, 4)\n</code></pre> <p>Another way to created a generator is with a generator expression.</p> Example of generator expression<pre><code>generator = (i for i in range(1, 4))\n\nfor value in generator: \n    print(value)\n</code></pre>","tags":["Design Patterns","Python","Data Analysis"]},{"location":"blog/posts/2023/iteration-pattern/#2-define-__iter__-and-__next__-methods","title":"2. Define <code>__iter__</code> and <code>__next__</code> methods","text":"<p>The <code>__iter__</code> method returns an iterator object and the <code>__next__</code> method returns the next value in the iterator. The <code>StopIteration</code> exception is raised when there are no more values to return.</p> <p>Note</p> <p>The class variables can be used to keep track of the current value and the max value.</p> Example of __iter__ and __next__ method<pre><code>class Iterator: \n    def __init__(self, max_value): \n        self.max_value = max_value\n        self.current_value = 0\n\n    def __iter__(self): \n        return self\n\n    def __next__(self): \n        if self.current_value &gt;= self.max_value: \n            raise StopIteration\n\n        self.current_value += 1\n        return self.current_value\n\nfor value in Iterator(3):\n    print(value)\n</code></pre> <pre><code>1\n2\n3\n</code></pre> <p>Note</p> <p>The return value of <code>__iter__</code> must be an iterator. This can be done by returning <code>self</code> or by defining a separate iterator class, generator function, or generator expression.</p>","tags":["Design Patterns","Python","Data Analysis"]},{"location":"blog/posts/2023/iteration-pattern/#which-to-use","title":"Which to Use?","text":"<p>Most of the time we are handed different data structures and wouldn't want to override the <code>__iter__</code> method. In this case, we can use generator functions to define the iteration method. </p> Sample Data Structure Handed to Us<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Matrix: \n    data: list[list[int]]\n\n    def __post_init__(self) -&gt; None: \n        assert self.nrows &gt; 0, \"Matrix must have at least one row\"\n        assert self.ncols &gt; 0, \"Matrix must have at least one column\"\n        # Matrix must be rectangular\n        assert all(len(row) == self.ncols for row in self.data), \"Matrix must be rectangular\"\n\n    @property \n    def nrows(self) -&gt; int: \n        return len(self.data)\n\n    @property \n    def ncols(self) -&gt; int: \n        return len(self.data[0])\n</code></pre> <p>Defining iteration outside of the object allows us to define different iteration methods. Any way we want to iterate over the matrix can be defined as a generator.</p> <p>Below are three different methods for the <code>Matrix</code> class. </p> <ol> <li>Row First</li> <li>Column First</li> <li>Diagonal</li> </ol> Define Iteration Methods<pre><code>def row_first_iteration(matrix: Matrix): \n    for row in matrix.data: \n        for value in row: \n            yield value \n\ndef column_first_iteration(matrix: Matrix): \n    for col in range(matrix.ncols): \n        for row in range(matrix.nrows): \n            yield matrix.data[row][col]\n\ndef diagonal_iteration(matrix: Matrix): \n    \"\"\"Iterate through the matrix diagonally. \n\n    Starts with the top left corner first and goes diagonally up.\n\n    \"\"\"\n    nrows = matrix.nrows\n    ncols = matrix.ncols\n\n    ndiags = ncols + nrows - 1\n\n    for diag in range(n_diags):\n        for col in range(diag + 1): \n            row = diag - col\n            if row &lt; nrows and col &lt; ncols: \n                yield matrix.data[row][col]\n</code></pre> <p>Now when we want to iterate over the matrix, we can choose which iteration method to use depending on the use case.</p> Example Usage<pre><code>data = [\n    [1, 2],\n    [3, 4], \n    [5, 6], \n    [7, 8]\n]\nmatrix = Matrix(data)\n\nrow_first_iter = row_first_iteration(matrix)\ncolumn_first_iter = column_first_iteration(matrix)\ndiag_iter = diagonal_iteration(matrix)\n\nimport pandas as pd \n\ndf = pd.DataFrame({\n    \"Row First\": list(row_first_iter),\n    \"Column First\": list(column_first_iter),\n    \"Diagonal\": list(diag_iter)\n})\ndf.index.name = \"Iteration\"\n</code></pre> <pre><code>           Row First  Column First  Diagonal\nIteration                                   \n0                  1             1         1\n1                  2             3         3\n2                  3             5         2\n3                  4             7         5\n4                  5             2         4\n5                  6             4         7\n6                  7             6         6\n7                  8             8         8\n</code></pre> <p>The <code>for</code> loop will be the same regardless of how we want to process the values:</p> Various ways to process values<pre><code>process = print\niterable = row_first_iter \n\nfor value in iterable:\n    process(value)\n\n# Alternative iteration and processing\ndef log_value_somewhere(value): \n    print(f\"Logging value {value}\")\n\nprocess = log_value_somewhere\niterable = diag_iter\n\nfor value in iterable: \n    process(value)\n</code></pre> <p>This can be useful when we want to process the values in different ways, but also from different data structures. </p> Example of different data structures<pre><code>import numpy as np\n\nmatrix_np = np.array(data)\n\niterable = iter(matrix_np.flatten())\n\nfor value in iterable: \n    process(value)\n</code></pre>","tags":["Design Patterns","Python","Data Analysis"]},{"location":"blog/posts/2023/iteration-pattern/#summary","title":"Summary","text":"<p>Classes are often handed to next user so it useful to define iteration methods outside of the class. Not only that, but what is done with the value is separated from the iteration method as well. That is, separation of: </p> <ol> <li>Data </li> <li>Iteration of data</li> <li>Processing of data</li> </ol> <p>This provides flexibility while keeping the code in a consistent format.</p> <pre><code>for value in iterable: \n    process(value)\n</code></pre>","tags":["Design Patterns","Python","Data Analysis"]},{"location":"blog/posts/2023/iteration-pattern/#references","title":"References","text":"<ul> <li>Python Generators</li> <li><code>__iter__</code> and <code>__next__</code></li> </ul>","tags":["Design Patterns","Python","Data Analysis"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/","title":"Strategy Pattern for Flexible Solutions","text":"","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/#strategy-pattern-for-flexible-solutions","title":"Strategy Pattern for Flexible Solutions","text":"<p>This is how the strategy design pattern was used to create a flexible solution for Marketing Mix Model (MMM) saturation and adstock functions in <code>pymc-marketing</code>.</p> <p>Marketing (Mad) Scientist, Carlos Agostini, and I worked on this solution in this pull request. Check there for more implementation details.</p> <p>Following this PR, there is built in support for 4 adstock functions and 5 saturation functions. Not only that, but the solution allows for easy addition of adstock, saturation ordering. In total, there are 4 * 5 * 2 = 40 new out-of-the-box MMM combinations with <code>pymc-marketing</code> not including the ability to add custom adstock and saturation functions as well.</p> Out-of-Box Adstock and Saturation Functions <p>Adstock</p> <ul> <li>Geometric</li> <li>Delayed</li> <li>Weibull CDF</li> <li>Weibull PDF</li> </ul> <p>Saturation</p> <ul> <li>Logistic</li> <li>Tanh</li> <li>Tanh Baselined</li> <li>Hill</li> <li>Michaelis-Menten</li> </ul>","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/#problem","title":"Problem","text":"<p>The <code>pymc-marketing</code> package requires flexibility in order to support a wide range of marketing assumptions including how media channels will contribute to the overall target metric.</p> <p>These assumptions affect:</p> <ul> <li>Adstock: how much of the effect of a channel is carried over to the next time   period.</li> <li>Saturation: diminishing returns as the media variable on a channel increases.</li> </ul> <p>This will focus on the saturation function and how the strategy pattern was used to create a flexible solution.</p> <p>Various saturation functions can be used to model diminishing returns. For instance, the following functions can be used:</p> <ul> <li>logistic</li> <li>tanh</li> <li>Michaelis-Menten</li> </ul> <p>On top of that, all of these functions have different sets of parameters. How might  all of these functions be used in a flexible way?</p>","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/#naive-approach","title":"Naive Approach","text":"<p>If there is only one saturation function, the problem is simple. For instance,</p> <pre><code>from numpy import exp\n\ndef logistic(x, lam): \n    return (1 - exp(-lam * x)) / (1 + exp(-lam * x))\n\ndef saturation(x, lam): \n    return logistic(x, lam)\n</code></pre> <p>However, as more functions are added, the problem becomes more complex. For instance, if a tanh function is added which has different parameters <code>b</code> and <code>c</code>, the <code>saturation</code> function would need to be updated.</p> <pre><code>from numpy import tanh\n\n# New function\ndef tanh_saturation(x, b, c): \n    return b * tanh(x / (b * c))\n\n# New logic\ndef saturation(x, lam=None, b=None, c=None): \n    if lam is not None: \n        return logistic(x, lam)\n    elif b is not None and c is not None: \n        return tanh_saturation(x, b, c)\n    else: \n        raise ValueError(\"Invalid parameters\")\n</code></pre> <p>The Michaelis-Menten function also has a parameter <code>lam</code>, so how would this be added to the <code>saturation</code> function?</p> <pre><code># Yet another new function\ndef michaelis_menten(x, lam, alpha): \n    return alpha * x / (x + lam)\n\n# New logic? Yet another parameter? Add a boolean flag?\ndef saturation(\n    x, \n    lam=None, \n    b=None, \n    c=None, \n    alpha=None,\n): \n    if lam is not None and alpha is None: \n        return logistic(x, lam)\n    if lam is not None and alpha is not None: \n        return michael_menten(x, lam, alpha)\n    elif b is not None and c is not None: \n        return tanh_saturation(x, b, c)\n    else: \n        raise ValueError(\"Invalid parameters\")\n</code></pre> <p>This approach quickly became unwieldy and hard to understand, scale, and maintain.</p> <p>Logic that is both flexible and can easily be extended without having to modify existing code is required!</p>","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/#strategy-pattern","title":"Strategy Pattern","text":"<p>The strategy pattern is a behavioral design pattern that can help solve this problem.</p>","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/#define-a-common-interface","title":"Define a Common Interface","text":"<p>By defining a common function signature, we can create this flexible system. For instance, we can pose the definition that a saturation function will always take a single argument and return a single value. </p> <pre><code>from typing import Callable\nfrom numpy.typing import ArrayLike \n\nSaturationFunction = Callable[[ArrayLike], ArrayLike]\n</code></pre> <p>The previously defined functions do not go to waste as they can be used to create functions that adhere to the <code>SaturationFunction</code> signature.</p> <pre><code>def create_logistic_saturation(lam) -&gt; SaturationFunction: \n    def saturation_function(x): \n        return logistic(x, lam)\n    return saturation_function\n\ndef create_tanh_saturation(b, c) -&gt; SaturationFunction: \n    def saturation_function(x):\n        return tanh_saturation(x, b, c)\n    return saturation_function\n\ndef create_michaelis_menten_saturation(lam, alpha) -&gt; SaturationFunction: \n    def saturation_function(x): \n        return michaelis_menten(x, lam, alpha)\n    return saturation_function\n</code></pre> <p>Each one of these functions is a wrapper that returns a function that adheres to the <code>SaturationFunction</code> signature. The parameters are passed in the wrapper function and the returned function only takes the media variable <code>x</code>.</p> <p>Tip</p> <p>The <code>functools.partial</code> function can also be used to create these functions</p> <pre><code>from functools import partial\n\nsaturation_function: SaturationFunction = partial(logistic, lam=0.1)\n</code></pre>","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/#two-step-process","title":"Two Step Process","text":"<p>By defining a wrapper function, this breaks the process into two steps: </p> <ol> <li>Creation</li> <li>Application</li> </ol> <pre><code># Creation\nsaturation = create_logistic_saturation(lam=0.1)\n\n# Application\nsaturation(0.5)\n</code></pre>","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/#benefits","title":"Benefits","text":"","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/#common-interface","title":"Common Interface","text":"<p>Switching to a different saturation function is as simple as changing the creation step because the application step remains the same.</p> <pre><code>saturation = create_tanh_saturation(b=0.1, c=0.2)\n\n# Same as before \nsaturation(0.5)\n</code></pre> <p>Even though the creation step is different, the application step remains the same.</p>","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/#extensibility","title":"Extensibility","text":"<p>There is also the benefit that new functions can be created without having to modify existing code. The defined signature ensures that the new function will work with the existing code.</p> <pre><code>def create_infinite_returns(beta) -&gt; SaturationFunction: \n    def saturation_function(x): \n        return beta * x\n    return saturation_function\n\ndef hill(x, sigma, beta, lam): \n    return sigma / (1 + exp(-beta * (x - lam)))\n\ndef create_hill_saturation(sigma, beta, lam) -&gt; SaturationFunction: \n    def saturation_function(x): \n        return hill(x, sigma, beta, lam)\n    return saturation_function\n</code></pre> <p>Even with user defined functions, the application step remains the same!</p> <pre><code># User defined saturation function\nsaturation = create_hill_saturation(sigma=0.1, beta=0.2, lam=0.3)\n\n# Still the same as before!\nsaturation(0.5)\n</code></pre>","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/#usage-example","title":"Usage Example","text":"<p>Because of the common interface, these created functions can be passed around making that naive <code>saturation</code> function no longer necessary.</p> <p>Below showcases all of the saturation functions in action which can be easily used due to the common interface.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Creation\nsaturation_functions = {\n    \"logistic\": create_logistic_saturation(lam=6),\n    \"tanh\": create_tanh_saturation(b=1.25, c=0.5),\n    \"Hill\": create_hill_saturation(sigma=1.5, beta=5.5, lam=0.75),\n    \"Michaelis-Menten\": create_michaelis_menten_saturation(lam=0.1, alpha=1.1),\n    \"Infinite Returns\": create_infinite_returns(beta=1.25),\n}\n\n# Application\nax = plt.subplot(111)\n\nx = np.linspace(0, 1, 100)\n\nfor name, saturation in saturation_functions.items(): \n    y = saturation(x)\n    ax.plot(x, y, label=name)\n\nax.legend()\nax.set(\n    xlabel=\"media variable\", \n    ylabel=\"saturated media variable\",\n    title=\"Strategy Pattern for Saturation Functions\",\n)\n</code></pre> <p></p>","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/#pymc-marketing-solution","title":"<code>pymc-marketing</code> Solution","text":"<p>There are a few additional requirements that are needed for the <code>pymc-marketing</code> solution. For instance, the need for:</p> <ul> <li>Prior for each function parameter</li> <li>Parameter estimation with <code>pymc</code></li> <li>Additional non-parameter arguments</li> <li>Non-clashing parameter names with larger model variables</li> <li>Lift test support</li> <li>Budget optimization</li> </ul> <p>The final solution ended up looking like this for each saturation function:</p> <pre><code>from pymc_marketing.mmm import SaturationTransformation\n\nclass InfiniteReturns(SaturationTransformation): \n    def function(self, x, alpha): \n        return alpha * x\n\n    default_priors = {\n       \"alpha\": {\n           \"dist\": \"HalfNormal\", \n           \"kwargs\": {\"sigma\": 0.1},\n       },\n    }\n</code></pre> <p>Though there is a little more boilerplate, the two step process is still used.</p> <ol> <li>Creation</li> <li>Application</li> </ol> <pre><code>saturation = InfiniteReturns()\n\nx = np.linspace(0, 1, 100)\n\nwith pm.Model():\n    saturated_x = saturation.apply(x)\n</code></pre> <p>There is much that comes for free with the <code>SaturationTransformation</code> class including the <code>apply</code> method which handles the common logic of creating PyMC distributions for the parameters of the <code>function</code> method based on <code>default_priors</code> while also ensuring a common interface for all saturation functions.</p> <p>Using this solution for the adstock and saturation functions in <code>pymc-marketing</code> provides the flexibility needed to support a wide range of marketing assumptions.</p> pymc-marketing solution in action<pre><code>from pymc_marketing.mmm import (\n    MMM,\n    MichaelisMentenSaturation \n    WeibullAdstock, \n)\n\n# Creation\nadstock = WeibullAdstock(kind=\"PDF\", l_max=7)\n\nsaturation_priors = {\n    \"lam\": {\"dist\": \"HalfNormal\", \"kwargs\": {\"sigma\": 0.1}},\n    \"alpha\": {\"dist\": \"Gamma\", \"kwargs\": {\"alpha\": 1, \"beta\": 2}},\n}\nsaturation = MichaelisMentenSaturation(priors=saturation_priors)\n\n# Application\nmmm = MMM(\n    ...,\n    adstock=adstock,\n    saturation=saturation,\n    ...,\n)\nmmm.fit(X, y)\n</code></pre>","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2024/pymc-marketing-strategy-pattern/#summary","title":"Summary","text":"<p>By defining a common function signature for all adstock and saturation functions, we get flexibility and ease addition of new functions without the need to update existing code. </p> <p>This breaks down the process into two steps:</p> <ol> <li>Creation</li> <li>Application</li> </ol> <p>This has been helpful to support a wide range of marketing assumptions required in <code>pymc-marketing</code>. </p>","tags":["Python","PyMC","Marketing","Design Patterns"]},{"location":"blog/posts/2025/automate-docs-with-marimo/","title":"Automate marimo Notebook Documentation","text":"","tags":["Python","Documentation","GitHub Actions"]},{"location":"blog/posts/2025/automate-docs-with-marimo/#automate-marimo-notebook-documentation","title":"Automate marimo Notebook Documentation","text":"<p>This is a follow up and alternative to using <code>mkdocs</code> for generating documentation: WASM-powered HTML via marimo</p> <p>I initially saw this method of documentation generation on Vincent Warmerdam's python package: <code>mohtml</code>. Much credit there for the inspiration.</p>","tags":["Python","Documentation","GitHub Actions"]},{"location":"blog/posts/2025/automate-docs-with-marimo/#setup","title":"Setup","text":"<p>This example requires:</p> <ol> <li>Python repository using <code>uv</code>: However, you can adapt this to any package manager.</li> <li>GitHub Pages enabled on the repository</li> </ol>","tags":["Python","Documentation","GitHub Actions"]},{"location":"blog/posts/2025/automate-docs-with-marimo/#steps","title":"Steps","text":"","tags":["Python","Documentation","GitHub Actions"]},{"location":"blog/posts/2025/automate-docs-with-marimo/#create-marimo-notebook","title":"Create marimo Notebook","text":"<p>The marimo notebook can be anything you like. In this example, the file is called <code>docs.py</code> and is in the root of the repository you are trying to generate the documentation for.</p> <p>Use <code>marimo edit docs.py</code> to create or edit the notebook to your liking.</p> <p>Here is a bare-bones example:</p> <pre><code>import marimo\n\n__generated_with = \"0.17.6\"\napp = marimo.App(width=\"medium\")\n\n\n@app.cell\ndef _():\n    import marimo as mo\n\n    mo.md(\"\"\"\n    ## My Documentation\n\n    Find all about my project documentation.\n    \"\"\")\n    return\n\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre>","tags":["Python","Documentation","GitHub Actions"]},{"location":"blog/posts/2025/automate-docs-with-marimo/#add-github-actions-workflow","title":"Add GitHub Actions Workflow","text":"<p>Here is the action workflow itself:</p> <pre><code># .github/workflows/docs.yml\n---\nname: Documentation\non:\n  push:\n    branches:\n      - main\n    paths:\n      # The path to your marimo notebook\n      - docs.py\n\njobs:\n  create-docs:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      # Checkout the repository\n      - uses: actions/checkout@v5\n\n      # Setup uv and install the project\n      - name: Install uv\n        uses: astral-sh/setup-uv@v7\n      - name: Install the project\n        run: uv sync --locked --all-extras\n\n      - name: Generate documentation\n        run: uv run marimo export html-wasm docs.py -o docs/index.html --mode run\n\n      - name: Deploy to GitHub Pages\n        uses: peaceiris/actions-gh-pages@v4\n        with:\n          github_token: {{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./docs\n          user_name: 'github-actions[bot]'\n          user_email: 'github-actions[bot]@users.noreply.github.com'\n</code></pre> <p>It will run on every push to main where there was change to <code>docs.py</code> file, generate the HTML, and deploys it to GitHub Pages on the <code>gh-pages</code> branch.</p>","tags":["Python","Documentation","GitHub Actions"]},{"location":"blog/posts/2025/automate-docs-with-marimo/#ensure-github-pages-is-set-up-correctly","title":"Ensure GitHub Pages is set up correctly","text":"<p>The default deployment branch is <code>gh-pages</code>. Ensure that your Pages setup uses this branch. You can check with the following command:</p> <pre><code>gh api /repos/:owner/:repo/pages    \n</code></pre> <p>Look for <code>.source.branch</code> and <code>.source.path</code> specifically. The expected output should look like this:</p> <pre><code>{\n  \"url\": \"https://api.github.com/repos/williambdean/frame-search/pages\",\n  \"status\": \"built\",\n  \"cname\": null,\n  \"custom_404\": false,\n  \"html_url\": \"https://williambdean.github.io/frame-search/\",\n  \"build_type\": \"legacy\",\n  \"source\": {\n    \"branch\": \"gh-pages\",\n    \"path\": \"/\"\n  },\n  \"public\": true,\n  \"protected_domain_state\": null,\n  \"pending_domain_unverified_at\": null,\n  \"https_enforced\": true\n}\n</code></pre> <p>You can create in the Settings &gt; Pages section on your repository if need be or create using the GitHub CLI.</p>","tags":["Python","Documentation","GitHub Actions"]},{"location":"blog/posts/2025/automate-docs-with-marimo/#result","title":"Result","text":"<p>The <code>main</code> branch will have two new files:</p> <ul> <li><code>docs.py</code></li> <li><code>.github/workflows/docs.yml</code></li> </ul> <p>Combined will result in deploying static WASM-powered HTML docs to the <code>gh-pages</code> branch.</p> <p>Link it in your repository and you are good to go!</p>","tags":["Python","Documentation","GitHub Actions"]},{"location":"blog/posts/2025/automate-docs-with-marimo/#more-information","title":"More Information","text":"<ul> <li>Check out my repository <code>frame-search</code> for a live implementation of this deployment.</li> <li>Gist of this GitHub Actions workflow.</li> <li>Read an alternative action on the marimo documentation</li> </ul>","tags":["Python","Documentation","GitHub Actions"]},{"location":"blog/posts/2025/pytest-debugging/","title":"Debugging with pytest","text":"","tags":["Python","Testing"]},{"location":"blog/posts/2025/pytest-debugging/#debugging-with-pytest","title":"Debugging with pytest","text":"<p>The <code>--pdb</code> flag in <code>pytest</code> is game changing for debugging tests. It allows you to drop into a debugger when a test fails. </p> <p>I've set up a command <code>DRunTests</code> to kick this off for the test that my cursor is on. </p> <p>If you would like to check around at a given point, use <code>assert 0</code> as an alternative. Here it is in action:</p> <p></p> <p>There are a few great commands to remember when using the debugger:</p> <ul> <li><code>p</code> to print variables</li> <li><code>pp</code> to pretty print variables</li> <li><code>q</code> to quit the debugger</li> <li><code>c</code> to continue running the test</li> </ul> <p>Tip</p> <p>When in doubt, use the <code>h(elp) expression</code> to get a list of all available commands.</p> <p>Does the pytest --pdb flag come in handy for you? Let me know in the comments below! </p>","tags":["Python","Testing"]}]}